# -*- coding: utf-8 -*-
"""Simple_Agent_and_Agentic_RAG_Example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Muv-t4dR1Ch-U0Wy5wV4lgdI6VFgoZ9
"""

from google.colab import userdata

# Access the key by its name
mistral_api_key = userdata.get('Mistral_API')

# Use the api_key variable in your API call
if mistral_api_key is not None:
    # Your API call code here
    pass
else:
    print("API key not found in Colab secrets.")

from langchain_core.tools import tool

@tool
def find_sum(x:int, y:int)->int:
  """This function is used to sum two numbers and return their product
  It takes two integers as inputs and returns an integer as output"""
  return x+y

@tool
def find_product(x:int, y:int)->int:
  """This function is used to multiply two numbers and return their products
  It takes two integers as inputs and return as integer as output"""
  return x*y

!pip install -qU langchain-mistralai

from langchain_mistralai import ChatMistralAI
models = ChatMistralAI(
    model="mistral-large-latest",
    temperature=0,
    api_key=mistral_api_key


)

from langchain_core.messages import AIMessage,HumanMessage, SystemMessage

agent_tools = [find_sum,find_product]

system_prompt = """You are a Math Genius who can solve math problems. Solve the problems provided by the user, by using only tools available, Do not solve the problem yourself"""

from langchain.agents import create_agent

agent = create_agent(
    model=models,
    system_prompt=system_prompt,
    tools = agent_tools
)

input = {"messages":[("user","What is the sum of 2 and 3?")]}
result = agent.invoke(input)

print(result['messages'][-1].content)

print("Step by Step Execution")
for message in result['messages']:
  print(message.pretty_repr())

input = {"messages":[("user","What is the 3 multiplied by 2 and 5+1")]}
result = agent.invoke(input)

print(result['messages'][-1].content)
print("Step by Step Execution")
for message in result['messages']:
  print(message.pretty_repr())

from langchain_mistralai import MistralAIEmbeddings
embeddings = MistralAIEmbeddings(
    model = "mistral-embed",
    api_key=mistral_api_key
)

"""# Add product pricing function tool"""

import pandas as pd
from langchain_core.tools import tool

product_pricing_df = pd.read_csv("/content/drive/MyDrive/Data_Sets/RAG_and_AgenticRAG/Laptop pricing.csv")

print(product_pricing_df)

@tool
def get_laptop_price(laptop_name : str)->int:
  """This function returns the price of the laptop, if you send it's name as input"""
  matched_record_df = product_pricing_df[product_pricing_df['Name'].str.contains("^"+laptop_name, case = False)]
  if len(matched_record_df) == 0:
    return -1
  else:
    return matched_record_df['Price'].iloc[0]

!pip install -qU "langchain-chroma>=0.1.2"
!pip install langchain_community
!pip install pypdf

from langchain.tools import tool
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

loader = PyPDFLoader("/content/drive/MyDrive/Data_Sets/RAG_and_AgenticRAG/Laptop product descriptions.pdf")

docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=256)
splits = text_splitter.split_documents(docs)

#Create a vector store with Chroma
prod_feature_store = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

retriever = prod_feature_store.as_retriever()

@tool
def get_product_features(query: str) -> str:
  """Search and return information about given product"""
  docs = retriever.invoke(query)
  return "\n\n".join([doc.page_content for doc in docs])

from langchain.agents import create_agent

from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import AIMessage,HumanMessage,SystemMessage

system_prompt = SystemMessage("""
    You are professional chatbot that answers questions about laptops sold by your company.
    To answer questions about laptops, you will ONLY use the available tools and NOT your own memory.
    You will handle small talk and greetings by producing professional responses.
    """
)

tools = [get_laptop_price, get_product_features]

checkpointer=MemorySaver()

#Create a Product QnA Agent. This is actual a graph in langGraph
product_QnA_agent=create_agent(
                                model=models, #LLM to use
                                tools=tools, #List of tools to use
                                system_prompt=system_prompt, #The system prompt
                                debug=False, #Debugging turned on if needed
                                checkpointer=checkpointer #For conversation memory
)

import uuid
#To maintain memory, each request should be in the context of a thread.
#Each user conversation will use a separate thread ID
config = {"configurable": {"thread_id": uuid.uuid4()}}

#Test the agent with an input
inputs = {"messages":[
                HumanMessage("What are the features and pricing for GammaAir?")
            ]}

#This is an alternate way to stream agent responses without waiting for the agent to finish
for stream in product_QnA_agent.stream(inputs, config, stream_mode="values"):
    message=stream["messages"][-1]
    if isinstance(message, tuple):
        print(message)
    else:
        message.pretty_print()

import uuid
#Send a sequence of messages to chatbot and get its response
#This simulates the conversation between the user and the Agentic chatbot
user_inputs = [
    "Hello",
    "I am looking to buy a laptop",
    "Give me a list of available laptop names",
    "Tell me about the features of  SpectraBook",
    "How much does it cost?",
    "Give me similar information about OmegaPro",
    "What info do you have on AcmeRight ?",
    "Thanks for the help"
]

#Create a new thread
config = {"configurable": {"thread_id": str(uuid.uuid4())}}

for input in user_inputs:
    print(f"----------------------------------------\nUSER : {input}")
    #Format the user message
    user_message = {"messages":[HumanMessage(input)]}
    #Get response from the agent
    ai_response = product_QnA_agent.invoke(user_message,config=config)
    #Print the response
    print(f"AGENT : {ai_response['messages'][-1].content}")

#conversation memory by user
def execute_prompt(user, config, prompt):
    inputs = {"messages":[("user",prompt)]}
    ai_response = product_QnA_agent.invoke(inputs,config=config)
    print(f"\n{user}: {ai_response['messages'][-1].content}")

#Create different session threads for 2 users
config_1 = {"configurable": {"thread_id": str(uuid.uuid4())}}
config_2 = {"configurable": {"thread_id": str(uuid.uuid4())}}

#Test both threads
execute_prompt("USER 1", config_1, "Tell me about the features of  SpectraBook")
execute_prompt("USER 2", config_2, "Tell me about the features of  GammaAir")
execute_prompt("USER 1", config_1, "What is its price ?")
execute_prompt("USER 2", config_2, "What is its price ?")































